
def calculate_dynamic_tolerance(key_length, tolerance_ratio=0.2):
    """ Calculate the dynamic tolerance based on the length of the key and a given tolerance ratio. """
    return max(1, int(round(key_length * tolerance_ratio)))  # Ensure at least 1 character can be different

def levenshtein_distance(s1, s2):
    """Compute the Levenshtein distance between two strings."""
    if len(s1) > len(s2):
        s1, s2 = s2, s1

    distances = range(len(s1) + 1)
    for index2, char2 in enumerate(s2):
        new_distances = [index2 + 1]
        for index1, char1 in enumerate(s1):
            if char1 == char2:
                new_distances.append(distances[index1])
            else:
                new_distances.append(1 + min((distances[index1], distances[index1 + 1], new_distances[-1])))
        distances = new_distances
    return distances[-1]
def calculate_metrics_with_dynamic_tolerance(ocr_results, manual_results, tolerance_ratio=0.2):
    # Convert list of dictionaries to dictionaries for easier access
    ocr_dict = {k: v for d in ocr_results for k, v in d.items()}
    manual_dict = {k.strip(): v for d in manual_results for k, v in d.items()}

    # Variables to hold match details
    true_positive = 0
    false_positive = 0
    false_negative = 0
    used_keys = set()

    # Check each OCR result against the manual results using edit distance for keys with dynamic tolerance
    for ocr_key, ocr_value in list(ocr_dict.items()):
        matched = False
        for manual_key in manual_dict.keys():
            if manual_key not in used_keys:
                calculated_tolerance = calculate_dynamic_tolerance(len(manual_key), tolerance_ratio)
                if levenshtein_distance(ocr_key, manual_key) <= calculated_tolerance:
                    # Compare values if keys are considered matching
                    if ocr_value == manual_dict[manual_key]:
                        true_positive += 1
                    else:
                        false_positive += 1
                    used_keys.add(manual_key)
                    matched = True
                    break
        if not matched:
            false_positive += 1

    # Remaining entries in manual_dict that were not used are false negatives
    false_negative = len(manual_dict) - len(used_keys)

    # Calculate precision, recall, and F1 score
    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return {
        "accuracy": true_positive / (true_positive + false_positive + false_negative) if (
                                                                                                 true_positive + false_positive + false_negative) > 0 else 0,
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score
    }
ocr_results = [{"yes": "blank"}, {"no12345678": "blank"}]
manual_results = [{"yes": "blank"}, {"no1234567891": "blank"}, {"other": "blank"}]
# Uncomment the line below to calculate the metrics when ready
a =  calculate_metrics_with_dynamic_tolerance(ocr_results, manual_results)
print(a)
import os
import json

def read_json_files(directory):
    """ Read all JSON files in the specified directory and return a dictionary of data. """
    data = {}
    for filename in os.listdir(directory):
        if filename.endswith(".json"):
            file_path = os.path.join(directory, filename)
            with open(file_path, 'r') as file:
                data[filename] = json.load(file)
    return data

def match_files(ocr_files, manual_files):
    """ Match OCR files with manual files based on filename patterns. """
    matches = {}
    for ocr_filename, ocr_data in ocr_files.items():
        # Assuming the manual files follow a naming convention like replacing 'ocr' with 'manual'
        manual_filename = ocr_filename.replace("ocr", "manual")
        if manual_filename in manual_files:
            matches[ocr_filename] = manual_filename
    return matches

def calculate_overall_metrics(ocr_directory, manual_directory):
    ocr_files = read_json_files(ocr_directory)
    manual_files = read_json_files(manual_directory)
    matched_files = match_files(ocr_files, manual_files)

    all_metrics = []
    for ocr_filename, manual_filename in matched_files.items():
        ocr_data = ocr_files[ocr_filename]
        manual_data = manual_files[manual_filename]
        metrics = calculate_metrics_with_dynamic_tolerance(ocr_data, manual_data)
        all_metrics.append(metrics)

    # Aggregate results
    if all_metrics:
        average_accuracy = sum(m['accuracy'] for m in all_metrics) / len(all_metrics)
        average_precision = sum(m['precision'] for m in all_metrics) / len(all_metrics)
        average_recall = sum(m['recall'] for m in all_metrics) / len(all_metrics)
        average_f1_score = sum(m['f1_score'] for m in all_metrics) / len(all_metrics)
        return {
            "average_accuracy": average_accuracy,
            "average_precision": average_precision,
            "average_recall": average_recall,
            "average_f1_score": average_f1_score
        }
    return {}


overall_metrics = calculate_overall_metrics("./ocr_dir", "./man_dir")
print(overall_metrics)
